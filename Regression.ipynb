{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c573567084cd4147",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Analyzing the World Happiness Data: Computing Linear Regressions Among Variables\n",
    "In this exercise, we will use scikit-learn to compute linear regressions among some of the variables in the World Happiness Report (WHR) data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-511e024cdd921aba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the scikit-learn linear model `LinearRegression`, the `train_test_split()` function for splitting the data into training and test sets, and the metrics `mean_squared_error` and `r2_score` to evaluate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bb9ab875260593a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1: Load & Process the Data\n",
    "\n",
    "Execute the next few code cells to load and reconfigure the data. Rather than examining the full dataset, we will just examine the data from 2015-2017, which we will store in a DataFrame named ```df1517```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b09dfe4a0153b39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dfraw = pd.read_excel('WHR2018Chapter2OnlineData.xls', sheet_name='Table2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae8c71db502ebc57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_to_include = ['country', 'year', 'Life Ladder', \n",
    "                   'Positive affect','Negative affect',\n",
    "                   'Log GDP per capita', 'Social support',\n",
    "                   'Healthy life expectancy at birth', \n",
    "                   'Freedom to make life choices', \n",
    "                   'Generosity', 'Perceptions of corruption']\n",
    "renaming = {'Life Ladder': 'Happiness', \n",
    "            'Log GDP per capita': 'LogGDP', \n",
    "            'Social support': 'Support', \n",
    "            'Healthy life expectancy at birth': 'Life', \n",
    "            'Freedom to make life choices': 'Freedom', \n",
    "            'Perceptions of corruption': 'Corruption', \n",
    "            'Positive affect': 'Positive', \n",
    "            'Negative affect': 'Negative'}\n",
    "df = dfraw[cols_to_include].rename(renaming, axis=1)\n",
    "df1517 = df[df.year.isin(range(2015,2018))]\n",
    "df1517 = df1517.dropna() # remove missing values\n",
    "df1517.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ded07cf9591d22ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The World Happiness Report is generally interested in how self-reported `Happiness` (Life Ladder) is dependent on the variety of different factors that they measure (`LogGDP`, `Support`, `Life`, etc.).  The report carries out a detailed analysis explaining how much of each country's `Happiness` can be ascribed to each of the explanatory factors.  We will consider later the specific analysis carried out in the WHR, but start here with a simpler analysis.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df7cce0d159e8950",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2: Visualize Variables of Interest\n",
    "\n",
    "Happiness and LogGDP in the WHR data are generally well correlated. Let's first examine a plot demonstrating this relationship. We will use the seaborn function `sns.regplot()` which will create a scatterplot of one feature `LogGDP` and one label `Happiness`, and overlay a line of best fit. \n",
    "\n",
    "Run the code cell below and examine the results. You'll notice a linear relationship between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61264c532bf2074e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='LogGDP', y='Happiness', data=df1517);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a341d16efe4ba567",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Visually, the relationship between the two variables plotted above looks something like a line, albeit with a fair amount of jitter above and below that line. \n",
    "\n",
    "Linear regression is a method that estimates a relationship between two variables by fitting a line to examples relating those variables.  That is, given a set of examples relating two variables, linear regression creates a <b>model</b> of the data by assuming that the data relationship is well described by a straight line &mdash; more specifically, a straight line *is* the model of the data.  Given our assumption that a line is a good description of the data relationship, we need to identify what is the specific line that best fits our particular dataset.\n",
    "\n",
    "Note: the features are also referred to as independent variables, and the label is also referred to as the dependent variable.\n",
    "\n",
    "Mathematically, a line relating an independent variable $x$ and a dependent variable $y$ is characterized by two parameters: the slope and the y-intercept.  Mathematically, we might write:\n",
    "\n",
    "$$y = \\alpha + w_1 x_1$$\n",
    "\n",
    "where $w_1$ represents the slope (or weight) and $\\alpha$ represents the y-intercept.  The y-intercept is where the line crosses the y-axis (i.e., for $x = 0$), and the slope indicates how a change $\\Delta x$ in the independent variable corresponds to a change $\\Delta y$ in the dependent variable (the slope is given $w_1 = \\Delta y / \\Delta x$).\n",
    "\n",
    "In our case, we are interested in quantifying the relationship between `Happiness` and `LogGDP`, so we are interested in a specific model of the form:\n",
    "\n",
    "$${\\rm Happiness} = \\alpha + w_1 * {\\rm LogGDP}$$\n",
    "\n",
    "Linear regression attempts to find the <b>best-fit line</b> that minimizes the least-squares error, that is, the squared difference between the actual training data's label and the model predicted label given by the equation above, summed over all examples. That is, linear regression produces a specific estimate for the model parameters $w_1$ and $\\alpha$ that does the best job of fitting the examples.\n",
    "\n",
    "Visually, we can see that the weight of the general trend in the data is approximately equal to 1, because the y-axis increases by around 4 units (`Happiness` increasing from approximately 3 to 7) at the same time that the x-axis also increases by around 4 units (`LogGDP` going from 7 to 11).  It is harder to estimate the intercept of the dataset, since at `LogGDP=7`, the data are far from the y-axis at $x = 0$.  You should recognize that some estimates for the model parameters $w_1$ and $\\alpha$ would do a poor job of describing the data.  If we chose the weight to be $w_1=100$, for example, then our model would be predicting a much more rapid rise than we see in the actual data.  Or if we chose a very large intercept such as $\\alpha=1000$, the model predictions would lie far above the data.  But we don't need to rely on visual inspection, since we can use tools to estimate these model parameters numerically.  (This is part of a general process typically referred to as \"parameter estimation\" or \"estimating parameters from data\", but it should be recognized that parameter estimation always occurs within the context of some assumed model, such as the straight line we are using here.)\n",
    "\n",
    "The method from scikit-learn that we will use specifically is called Ordinary Least Squares (OLS), which is the simplest of linear regression methods to estimate the model parameters that minimize the mean-squared error (MSE) between the actual data and the model predictions. The MSE is computed as:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $y_i$ is the $i$th example's label and $\\hat{y}_i$ is the regression model's predicted value. OLS chooses to minimize the MSE loss function. There are many possible loss functions for a regression problem and they will all give different results. The MSE is one of the simplest and most theoretically understood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Labeled Examples from the Data Set \n",
    "\n",
    "\n",
    "We will create a <b>simple linear regression</b> model that finds the linear relationship between one feature and one label. We will use the `LogGDP` feature and the `Happiness` label. Our model will predict a `Happiness` value for a given `LogGDP` value.\n",
    "\n",
    "Let's extract these columns from our data set to create labeled examples. The code cell below carries out the following steps:\n",
    "\n",
    "* Extracts the `LogGDP` column from DataFrame ```df1517``` and assigns it to the variable ```X```. This will be our feature. **The code uses the scikit-learn```to_frame()``` method on `X` to keep it as a Pandas DataFrame instead of a series. This is important for the LinearRegression `fit()` method in scikit-learn.**\n",
    "* Extracts the `Happiness` column from DataFrame ```df1517``` and assigns it to the variable ```y```. This will be our label.\n",
    "* Prints the values of `X` and `y`\n",
    "\n",
    "\n",
    "Execute the code cell below and inspect the results. You will see that we have 380 labeled examples. Each example contains one feature (`LogGDP`) and one label (`Happiness`).\n",
    "\n",
    "Our goal will be to fit a straight line relating the data in ```X``` to the data in ```y```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-631253ec2dee65d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = df1517['LogGDP'].to_frame()\n",
    "y = df1517['Happiness']\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Training and Test Data Sets\n",
    "\n",
    "Now that we have specified examples, we will need to split them into a training set, which we will use to estimate the model parameters $w_1$ and $\\alpha$, and a test set, which we will use to understand the performance of our model on new data. \n",
    "\n",
    "\n",
    "Run the code cell below to run the ```train_test_split()``` function with `X` and `y` as inputs, along with the parameters:\n",
    "* `random_state=42` to ensure reproducible output each time the function is called\n",
    "* `test_size=0.15`, which will randomly set aside 15% of the data to be used for testing. \n",
    "\n",
    "The function ```train_test_split()``` will return four outputs that we have assigned to variables `X_train`, `X_test`, `y_train`, `y_test`. \n",
    "\n",
    "\n",
    "Note that `X_train` corresponds to features and `y_train` corresponds to labels. Likewise, `X_test` corresponds to the features we will use to test our model, and `y_test` corresponds to the labels that we will use to determine whether our model's predictions were accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0efe0faded49a4a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 5: Fit a Linear Regression Model with the Training Set\n",
    "\n",
    "Now let's carry out the regression.  Scikit-learn works by creating objects that represent a model of the data: in this case, the model is linear regression, or a least-squares straight-line fit to the data. First let's create a ```LinearRegression``` model object, and then fit the model to the training data, which is the process by which the best-fit model parameters are estimated.\n",
    "\n",
    "The code cell below:\n",
    "\n",
    "1. Creates a ```LinearRegression``` model object and assigns the result to the variable ```model```.  You might find it useful to consult the [scikit-learn documentation on LinearRegression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares).\n",
    "2. Calls the ```model.fit()``` method to fit the model to the training data. \n",
    "3. The cell below uses the ```model.predict()``` method with the argument ```X_test``` to use the trained linear regression model to predict values for the test data. It stores the outcome in the variable ```prediction```. We will compare these values to ```y_test``` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0b7c721040f349a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the  LinearRegression model object \n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#  Make predictions on the test data \n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the model parameters that were determined during training.\n",
    "\n",
    "* The weights $w$ are stored in numpy array ```model.coef_```. To access a particular weight, such as $w_1$, index into the array using the notation ```model.coef_[0]```. Note that we are using index notation, so the weight of feature 1 is found in element 0 of the NumPy array, the weight of feature 2 is found in element 1 of the NumPy array, and so on.\n",
    "* The intercept $\\alpha$ is stored in ```model.intercept_```. \n",
    "\n",
    "Run the cell below and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight_1 (weight of feature LogGDP)\n",
    "print('Model Summary\\n\\nWeight_1 =  ', model.coef_[0], '[ weight of feature LogGDP ]')\n",
    "# alpha\n",
    "print('Alpha = ', model.intercept_, '[ intercept ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can translate the output above the following way: the training phase has identified that the best straight-line model fitting `Happiness` to `LogGDP` is approximately:\n",
    "\n",
    "$${\\rm Happiness} = 0.7743 * {\\rm LogGDP} - 1.7507$$\n",
    "\n",
    "In the visual estimation of the slope that was done in <b>Step 2</b>, we noted that the slope should be approximately equal to 1.  That turned out to be an over-estimate since the best-fit value is actually around 0.77, but it's not wildly off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-906322e778ea00dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 6: Evaluate the Model on the Test Set\n",
    "\n",
    "Now that we have trained the model and made some predictions we will want to examine how well the model performed.\n",
    "\n",
    "To evaluate our model, we will compute the RMSE (root mean square error) on the test set. RMSE is a metric used to evaluate Regression models. Root Mean Square Error (RMSE) finds the differences between the predicted values and the actual values. \n",
    "\n",
    "To compute the RMSE, we will use the scikit-learn ```mean_squared_error()``` function, which computes the MSE between ```y_test``` and ```prediction```. We will then take the square root of the result to obtain the RMSE. \n",
    "\n",
    "Finally, we will use the coefficient of determination, also known as $R^2$. $R^2$ is a measure of the proportion of variability in the prediction that the model was able to make using the input data. An $R^2$ value of 1 is perfect and 0 implies no explanatory value. We can use scikit-learn's ```r2_score()``` function to compute it. Run the code below and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean squared error\n",
    "print('\\nModel Performance\\n\\nRMSE =   %.2f'\n",
    "      % np.sqrt(mean_squared_error(y_test, prediction)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(' R^2 =   %.2f'\n",
    "      % r2_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the evaluation metrics, we have an RMSE of 0.71. This means that, on average, our predictions are off by 0.71 units. Since the `Happiness` feature in our data set ranges between about 2.5 and 8, this result is not bad! To truly evaluate this we would want to compare this result to the RMSE when using another simpler model, such as using the mean value of the `Happiness` feature as our prediction for every value of ```LogGDP```. \n",
    "\n",
    "The $R^2$ value of 0.62 implies that 62% of the variation in the ```Happiness``` feature was explained with the model by variation in ```LogGDP```. There is some subjectivity to interpreting what value is sufficient to justify the use of the model here, but let's just say that in the social sciences, it could also be a lot worse than 56%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7bde51abecad6399",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 7: Visualize the Model\n",
    "\n",
    "We can plot the data and the fit together using Matplotlib. The code cell below:\n",
    "\n",
    "* Uses ```plt.scatter()``` to plot ```X``` and ```y```. It sets the point size s to a reasonable value to prevent overplotting of points on top of each other.\n",
    "* Uses ```plt.plot()``` to add a blue line to the plot using the values in ```X_test``` and ```prediction```.\n",
    "* Uses ```plt.xlabel()``` and ```plt.ylabel()``` to label the axes appropriately. \n",
    "* The plot should look similar to the ```sns.regplot()``` created earlier in Step 2. \n",
    "\n",
    "Execute the code cell below and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7fb25c84b0fc211e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y,  color='black',s=15);\n",
    "\n",
    "plt.plot(X_test, prediction, color='blue', linewidth=3);\n",
    "\n",
    "plt.xlabel('LogGDP');\n",
    "plt.ylabel('Happiness');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c9ce2bb379b26aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 8: Create Labeled Examples for Multiple Linear Regression\n",
    "\n",
    "Simple linear regression finds the linear relationship between one feature and one label, and multiple regression finds the linear relationship between multiple features and one label. \n",
    "\n",
    "We just performed a simple linear regression and found a relationship between `LogGDP` and `Happiness`.\n",
    "\n",
    "But we are not just interested in how `Happiness` depends on `LogGDP`, but how it relates to the full set of features collected in the WHR data: `LogGDP`, `Support`, `Life`, `Freedom`, `Generosity`, and `Corruption`.  To analyze this full set of dependencies, we can set up a multiple linear regression, which aims to fit a label $y$ to a group of features $X$, by assuming that $y$ depends on each individual feature $X_i$ separately and in a linear manner.  Instead of a single weight $w_1$ as in the simple regression problem, there will now be a separate weight for each feature, i.e.:\n",
    "\n",
    "$${\\rm Happiness} =\\alpha + [w_1 * {\\rm LogGDP}] + [w_2 * {\\rm Support}] + [w_3 * {\\rm Life}] +[w_4 * {\\rm Freedom}] + [w_5 * {\\rm Generosity}] + [w_6 * {\\rm Corruption}]$$\n",
    "\n",
    "The code cell below creates the labeled examples for the multiple regression problem, similar as to what we did above. It performs the following tasks:\n",
    "\n",
    "* Assigns to the variable ```features``` the list of column names for the features of interest.\n",
    "* Assigns to the variable ```y``` the `Happiness` columns in the ```df1517``` DataFrame.\n",
    "* Assigns to the variable ```X``` the columns in ```df1517``` that are listed in `features` . **Since you are extracting multiple columns from ```df1517``` the result will be a DataFrame. You do not have to coerce it back to one like you did before using .to_frame()**\n",
    "* Prints the value of ```X``` to verify that it has been constructed correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03b7b6d12d51d97a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "features = ['LogGDP', 'Support', 'Life', 'Freedom', 'Generosity', 'Corruption']\n",
    "\n",
    "X = df1517[features]\n",
    "y = df1517['Happiness']\n",
    "\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create Training and Test Data Sets\n",
    "\n",
    "\n",
    "The code cell below calls `train_test_split()` on ```X``` and ```y```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f1c4feb3875d5cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 10: Fit a Multiple Linear Regression Model\n",
    "\n",
    "Similar to what we did above, now let's create an OLS model for the multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d4d67b47ea53cc71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the  LinearRegression model object \n",
    "model2 = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data \n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "#  Make predictions on the test data \n",
    "prediction2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Summary:\\n')\n",
    "\n",
    "# Print intercept (alpha)\n",
    "print('Intercept:')\n",
    "print('alpha = ' , model2.intercept_)\n",
    "\n",
    "# Print weights\n",
    "print('\\nWeights:')\n",
    "i = 0\n",
    "for w in model2.coef_:\n",
    "    print('w_',i+1,'= ', w, ' [ weight of ', features[i],']')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate the Model on the Test Set\n",
    "\n",
    "Run the cell below to examine the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean squared error\n",
    "print('\\nModel Performance\\n\\nRMSE =   %.2f'\n",
    "      % np.sqrt(mean_squared_error(y_test, prediction2)))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(' R^2 =   %.2f'\n",
    "      % r2_score(y_test, prediction2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90ec6b9cc1347da7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 12: Conclusions\n",
    "\n",
    "Examine the output in the Model Summary.  Estimates of model parameters are now provided for all of the features, as well as the overall intercept.  Note that the estimates for the intercept and the `LogGDP` weight are different than was the case in the simple regression.  That is typical, since multiple regression accounts for relationships between each independent and dependent variable once all the other data relationships are taken into account.\n",
    "\n",
    "We can see from the summary results that the `Support` and `Freedom` variables have the largest weights, indicating that, on average, an increase in those variables corresponds to an increase in `Happiness`. `Corruption` has a negative weight implying that, on average, a decrease in that feature corresponds to an increase in `Happiness`. These results fit with our common sense which is always important to verify.\n",
    "\n",
    "We also see that our RMSE has decreased and our $R^2$ value has increased, both good indicators that adding more features has increased the accuracy and fit of the model (although it is important to note that adding variables will always increase $R^2$ and that the magnitude of the increase may differ depending on the variable)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive: Iterative Approach - Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is a non-iterative linear regression. If you have a regression problem that will benefit from using the iterative approach that uses the optimization algorithm Gradient Descent, scikit-learn makes it easy for you. You can simply use the `SGDRegressor` class in place of the `LinearRegression` class. (Note that `SGDRegressor` uses a type of Gradient Descent algorithm called Stochastic Gradient Descent). \n",
    "\n",
    "Essentially, you would replace the line ``model = LinearRegression()`` with the line ``model = SGDRegressor(loss='squared_loss', max_iter=1000, tol=1e-3, learning_rate='constant')``, but supplying the arguments of your choosing. You'll note that `SGDRegressor` allows you to specify which loss function to use, and the max number of iterations over your training data (epochs). It will also allow you to set hyperparameters, such as the `learning rate`. Once you train your model, you can evaluate the model's performance, and run the `SGDRegressor` again with different hyperparameter arguments.\n",
    "\n",
    "You can consult the [scikit-learn documentation on SGDRegressor](https://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd) for more information. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
